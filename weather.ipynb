{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "南投市, 南投縣, 臺灣 每日天氣 | AccuWeather\n",
      "[{\"day\": \"周日\", \"date\": \" 6/20\", \"H_temp\": \" 31°\", \"L_temp\": \" 25°\", \"rain\": \" 52%\", \"UV\": \"6\"}, {\"day\": \"周一\", \"date\": \" 6/21\", \"H_temp\": \" 31°\", \"L_temp\": \" 25°\", \"rain\": \" 56%\", \"UV\": \"4\"}, {\"day\": \"周二\", \"date\": \" 6/22\", \"H_temp\": \" 29°\", \"L_temp\": \" 24°\", \"rain\": \" 66%\", \"UV\": \"4\"}, {\"day\": \"周三\", \"date\": \" 6/23\", \"H_temp\": \" 28°\", \"L_temp\": \" 23°\", \"rain\": \" 88%\", \"UV\": \"3\"}, {\"day\": \"周四\", \"date\": \" 6/24\", \"H_temp\": \" 28°\", \"L_temp\": \" 24°\", \"rain\": \" 85%\", \"UV\": \"4\"}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#import os\n",
    "#import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "#import string\n",
    "import json\n",
    "\n",
    "##\n",
    "punctuation=['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"]\n",
    "\n",
    "playday = \"2021-6-20\"\n",
    "duration = 5\n",
    "location = '南投市'\n",
    "\n",
    "now_str = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "now = datetime.datetime.strptime(now_str, \"%Y-%m-%d\")\n",
    "future = datetime.datetime.strptime(playday, \"%Y-%m-%d\")\n",
    "days_diff = (future - now).days\n",
    "days_end = days_diff + duration\n",
    "\n",
    "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'}\n",
    "#url = 'https://www.accuweather.com/zh/tw/east-district/2515456/daily-weather-forecast/2515456'\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "my_params = {'q': location+' daily accuweather'}\n",
    "html_data =requests.get(url=url,headers=headers,params = my_params)\n",
    "#print(html_data.url)\n",
    "soup=BeautifulSoup(html_data.text,\"html.parser\")\n",
    "#print(soup.prettify())\n",
    "daily =soup.select_one('div.yuRUbf a')\n",
    "#print(daily['href'])\n",
    "\n",
    "daily_data0 = requests.get(url=daily['href']+'?page=0',headers=headers)\n",
    "print(daily_data0)\n",
    "daily_data1 = requests.get(url=daily['href']+'?page=1',headers=headers)\n",
    "daily_data2 = requests.get(url=daily['href']+'?page=2',headers=headers)\n",
    "\n",
    "soup=BeautifulSoup(daily_data0.text,\"html.parser\")\n",
    "title_tag = str(soup.title.text)\n",
    "print(title_tag)\n",
    "\n",
    "divs_text=[]\n",
    "\n",
    "divs = soup.find_all('div', 'daily-wrapper')\n",
    "for div in divs:\n",
    "    divs_text.append(str([s for s in div.stripped_strings]))\n",
    "    \n",
    "soup=BeautifulSoup(daily_data1.text,\"html.parser\")\n",
    "divs = soup.find_all('div', 'daily-wrapper')\n",
    "for div in divs:\n",
    "    divs_text.append(str([s for s in div.stripped_strings]))\n",
    "    \n",
    "soup=BeautifulSoup(daily_data2.text,\"html.parser\")\n",
    "divs = soup.find_all('div', 'daily-wrapper')\n",
    "for div in divs:\n",
    "    divs_text.append(str([s for s in div.stripped_strings]))\n",
    "\n",
    "# jsonArr = json.dumps(divs_text,ensure_ascii=False)\n",
    "# print(jsonArr)\n",
    "    \n",
    "    \n",
    "## data processing (pandas)\n",
    "\n",
    "df = pd.DataFrame(data = divs_text)\n",
    "\n",
    "df.columns = [\"day\"]\n",
    "\n",
    "df = df['day'].str.split(',', expand=True)\n",
    "df = df.drop(4, axis = 1)\n",
    "df = df.drop(6, axis = 1)\n",
    "\n",
    "df.columns = ['day','date','H_temp','L_temp',\"rain\"]\n",
    "\n",
    "## ultraviolet\n",
    "\n",
    "Ultraviolet=[]\n",
    "\n",
    "for days in range(days_diff,days_end):\n",
    "    url =daily['href']+'?day=%s'%(days+1)\n",
    "    u_data = requests.get(url=url,headers=headers)\n",
    "    soup=BeautifulSoup(u_data.text,\"html.parser\")\n",
    "    divs = soup.select_one('div.panels div p span').text\n",
    "    Ultraviolet.append(str(divs))\n",
    "\n",
    "df1 = pd.DataFrame(data = Ultraviolet)\n",
    "df1.columns= ['UV']\n",
    "df1.index = range(days_diff,days_end)\n",
    "df1=df1.UV.str.extract('(\\d+)')\n",
    "df1.columns= ['UV']\n",
    "\n",
    "#print(df1)\n",
    "\n",
    "##\n",
    "punct = '!\"#$&\\'/()*+,-.:;<=>?@[\\\\]^_`{}~'   # `|` is not present here\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "df['day'] = '|'.join(df['day'].tolist()).translate(transtab).split('|')\n",
    "df['H_temp'] = '|'.join(df['H_temp'].tolist()).translate(transtab).split('|')\n",
    "df['L_temp'] = '|'.join(df['L_temp'].tolist()).translate(transtab).split('|')\n",
    "df['rain'] = '|'.join(df['rain'].tolist()).translate(transtab).split('|')\n",
    "\n",
    "punct = '\\':'\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "df['date'] = '|'.join(df['date'].tolist()).translate(transtab).split('|')\n",
    "\n",
    "df=df.iloc[days_diff:days_end]\n",
    "merged_df = pd.concat([df, df1], axis=1)\n",
    "merged_df.index = range(len(merged_df))\n",
    "\n",
    "## pd into dict to json\n",
    "merged_df_dict = merged_df.to_dict('records')\n",
    "#print(merged_df_dict)\n",
    "# merged_df_list= []\n",
    "# merged_df_list.append(merged_df_dict)\n",
    "#print(merged_df_list)\n",
    "jsonArr = json.dumps(merged_df_dict,ensure_ascii=False)\n",
    "print(jsonArr)\n",
    "\n",
    "\n",
    "## CSV\n",
    "# out_dir = './desktop'\n",
    "# out_name = 'weather_%s.csv'% location\n",
    "# if not os.path.exists(out_dir):\n",
    "#     os.mkdir(out_dir)\n",
    "# fullname = os.path.join(out_dir, out_name)    \n",
    "# merged_df.to_csv(fullname, encoding='utf_8_sig', index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://weather.com/weather/tenday/l/Ren+ai+Nantou+County?canonicalCityId=d6f8f98303cc3b592ece4131eecf7bca35158060dce8be46c7976852a5431116\n",
      "Renai Township, Nantou\n",
      "     date H_temp L_temp         weather  rain       UV\n",
      "2  Mon 21    21°    16°   Thunderstorms   90%  5 of 10\n",
      "3  Tue 22    21°    16°   Thunderstorms   96%  5 of 10\n",
      "4  Wed 23    20°    15°   Thunderstorms   98%  5 of 10\n",
      "5  Thu 24    21°    15°            Rain   78%  8 of 10\n",
      "6  Fri 25    23°    15°   Thunderstorms   73%  Extreme\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#import os\n",
    "#import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import string\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "punctuation=['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"]\n",
    "\n",
    "playday = \"2021-6-22\"\n",
    "duration = 5\n",
    "location = 'Renai Township, Nantou County'\n",
    "\n",
    "now_str = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "now = datetime.datetime.strptime(now_str, \"%Y-%m-%d\")\n",
    "future = datetime.datetime.strptime(playday, \"%Y-%m-%d\")\n",
    "days_diff = (future - now).days\n",
    "days_end = days_diff + duration\n",
    "\n",
    "headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.80 Safari/537.36'}\n",
    "url = 'https://www.google.com/search'\n",
    "\n",
    "my_params = {'q': location+' weather.com/weather/tenday'}\n",
    "html_data =requests.get(url=url,headers=headers,params = my_params)\n",
    "#print(html_data.url)\n",
    "soup=BeautifulSoup(html_data.text,\"html.parser\")\n",
    "#print(soup.prettify())\n",
    "daily =soup.select_one('div.yuRUbf a')\n",
    "print(daily['href'])\n",
    "\n",
    "daily_data0 = requests.get(url=daily['href'],headers=headers)\n",
    "#print(daily_data0.url)\n",
    "\n",
    "soup=BeautifulSoup(daily_data0.text,\"html.parser\")\n",
    "#title_tag = str(soup.title.text)\n",
    "title_tag = soup.select_one('div.removeIfEmpty h1 span span')\n",
    "print(title_tag.text)\n",
    "\n",
    "divs_text=[]\n",
    "UVs_text=[]\n",
    "\n",
    "UVs = soup.find_all('span',{'data-testid':'UVIndexValue'})\n",
    "for UV in UVs:\n",
    "        UVs_text.append(str([s for s in UV.stripped_strings]))\n",
    "for i in range(15):\n",
    "    id = str(\"titleIndex\"+str(i))\n",
    "    divs = soup.find(id=id)\n",
    "    for div in divs:\n",
    "        divs_text.append(str([s for s in div.stripped_strings]))\n",
    "\n",
    "#print(UVs_text)\n",
    "\n",
    "index0=[]\n",
    "index1=[]\n",
    "index2=[]\n",
    "index3=[]\n",
    "index4=[]\n",
    "\n",
    "for i in range(len(divs_text)):\n",
    "    if (i%5==0):\n",
    "        index0.append(divs_text[i])\n",
    "    elif(i%5==1):\n",
    "        index1.append(divs_text[i])\n",
    "    elif(i%5==2):\n",
    "        index2.append(divs_text[i])\n",
    "    elif(i%5==3):\n",
    "        index3.append(divs_text[i])\n",
    "    elif(i%5==4):\n",
    "        index4.append(divs_text[i])\n",
    "\n",
    "## data processing (pandas)\n",
    "\n",
    "df0 = pd.DataFrame(data = index0)\n",
    "df1 = pd.DataFrame(data = index1)\n",
    "df2 = pd.DataFrame(data = index2)\n",
    "df3 = pd.DataFrame(data = index3)\n",
    "df4 = pd.DataFrame(data = index4)\n",
    "df5 = pd.DataFrame(data = UVs_text)\n",
    "df5.columns = [\"d\"]\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    if(i==0 and df5[\"d\"][0]==\"['0 of 10']\"):\n",
    "        for i in range(1,30):\n",
    "            if(i%2==0):\n",
    "                df5 = df5.drop([i])\n",
    "    if(i==1 and df5[\"d\"][1]==\"['0 of 10']\"):\n",
    "        df5.loc[29] =\"['Extreme']\"\n",
    "        df5 = df5.drop([i])\n",
    "        for i in range(2,30):\n",
    "            if(i%2==1):\n",
    "                df5 = df5.drop([i])\n",
    "df5.index = range(len(df5))\n",
    "\n",
    "\n",
    "merged_df = pd.concat([df0,df1,df2,df3,df4,df5], axis=1)\n",
    "#print(merged_df)\n",
    "\n",
    "merged_df.columns = [\"day\",\"temp\",\"weather\",\"rain\",\"other\",\"UV\"]\n",
    "final_df = pd.concat([df0,df1,df2,df3,df4,df5], axis=1)\n",
    "final_df.columns = [\"date\",\"H_temp\",\"L_temp\",\"weather\",\"rain\",\"UV\"]\n",
    "\n",
    "\n",
    "temp0_df = pd.concat([df0,df1], axis=1)\n",
    "temp0_df.columns = [\"0\",\"1\"]\n",
    "temp1_df = pd.concat([df0,df1], axis=1)\n",
    "temp1_df.columns = [\"0\",\"1\"]\n",
    "\n",
    "\n",
    "final_df[['H_temp','L_temp']]= merged_df['temp'].str.split(',', expand=True).iloc[:,[0,2]]\n",
    "final_df['weather']= merged_df['weather'].str.split(',', expand=True).iloc[:,1]\n",
    "final_df['rain']= merged_df['rain'].str.split(',', expand=True).iloc[:,1]\n",
    "\n",
    "\n",
    "##\n",
    "punct = '!\"#$&\\'/()*+,-.:;<=>?@[\\\\]^_`{}~'   # `|` is not present here\n",
    "transtab = str.maketrans(dict.fromkeys(punct, ''))\n",
    "merged_df['temp'] = '|'.join(merged_df['temp'].tolist()).translate(transtab).split('|')\n",
    "#print(merged_df)\n",
    "final_df['date'] = '|'.join(final_df['date'].tolist()).translate(transtab).split('|')\n",
    "final_df['H_temp'] = '|'.join(final_df['H_temp'].tolist()).translate(transtab).split('|')\n",
    "final_df['L_temp'] = '|'.join(final_df['L_temp'].tolist()).translate(transtab).split('|')\n",
    "final_df['weather'] = '|'.join(final_df['weather'].tolist()).translate(transtab).split('|')\n",
    "final_df['rain'] = '|'.join(final_df['rain'].tolist()).translate(transtab).split('|')\n",
    "final_df['UV'] = '|'.join(final_df['UV'].tolist()).translate(transtab).split('|')\n",
    "\n",
    "\n",
    "temp0_df[['0','1']]= final_df['H_temp'].str.split('°', expand=True)\n",
    "temp1_df[['0','1']]= final_df['L_temp'].str.split('°', expand=True)\n",
    "\n",
    "if(temp0_df['1'][0]==None):\n",
    "    temp0_df['0'][0] = temp1_df['0'][0]\n",
    "\n",
    "\n",
    "for i in temp0_df.index:\n",
    "    temp0_df['0'][i] = str(round((int(temp0_df['0'][i])-32)*5/9))\n",
    "    temp1_df['0'][i] = str(round((int(temp1_df['0'][i])-32)*5/9))\n",
    "    temp0_df['1'][i] = '°'\n",
    "    temp1_df['1'][i] = '°'\n",
    "\n",
    "\n",
    "temp0_df= temp0_df['0'].str.cat(temp0_df['1'],sep=\"\")\n",
    "temp1_df= temp1_df['0'].str.cat(temp1_df['1'],sep=\"\")\n",
    "temp0_df.columns =[\"H_temp\"]\n",
    "temp1_df.columns =[\"L_temp\"]\n",
    "\n",
    "final_df[\"H_temp\"] = temp0_df\n",
    "final_df[\"L_temp\"] = temp1_df\n",
    "\n",
    "\n",
    "\n",
    "final_df= final_df.iloc[days_diff:days_end]\n",
    "\n",
    "print(final_df)\n",
    "\n",
    "final_df_dict = final_df.to_dict('records')\n",
    "jsonArr = json.dumps(final_df_dict,ensure_ascii=False)\n",
    "#print(jsonArr)\n",
    "\n",
    "\n",
    "\n",
    "## CSV\n",
    "# out_dir = './desktop'\n",
    "# out_name = 'weather_%s.csv'% location\n",
    "# if not os.path.exists(out_dir):\n",
    "#     os.mkdir(out_dir)\n",
    "# fullname = os.path.join(out_dir, out_name)    \n",
    "# merged_df.to_csv(fullname, encoding='utf_8_sig', index=False) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
